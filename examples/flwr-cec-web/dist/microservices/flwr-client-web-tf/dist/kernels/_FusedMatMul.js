/*! For license information please see _FusedMatMul.js.LICENSE.txt */
import{_FusedMatMul,broadcast_util}from"@tensorflow/tfjs-core";import{FusableActivation}from"./types";let wasmFusedMatMul;function setup(a){wasmFusedMatMul=a.wasm.cwrap(_FusedMatMul,null,["number","array","number","number","array","number","number","number","number","number","number","number","number"])}function fusedBatchMatMul(a){const{inputs:t,backend:e,attrs:n}=a,{a:r,b:s,bias:u,preluActivationWeights:d}=t;if("float32"!==r.dtype||"float32"!==s.dtype)throw new Error("_FusedMatMul for non non-float32 tensors not yet supported.");const{transposeA:o,transposeB:l,activation:p,leakyreluAlpha:i}=n,c=e.dataIdMap.get(r.dataId).id,M=e.dataIdMap.get(s.dataId).id;let h=0;if(null!=u){const a=e.dataIdMap.get(u.dataId);if(1!==a.shape.length)throw new Error(`_FusedMatMul only supports rank-1 bias but got rank ${a.shape.length}.`);h=a.id}const b=null==d?0:e.dataIdMap.get(d.dataId).id,m=FusableActivation[p];if(null==m)throw new Error(`${p} activation not yet supported for FusedConv2D in the wasm backend.`);const f=o?r.shape[2]:r.shape[1],w=l?s.shape[1]:s.shape[2],y=broadcast_util.assertAndGetBroadcastShape(r.shape.slice(0,-2),s.shape.slice(0,-2)),F=e.makeOutput([...y,f,w],r.dtype),g=e.dataIdMap.get(F.dataId).id,I=new Uint8Array(new Int32Array(r.shape).buffer),A=new Uint8Array(new Int32Array(s.shape).buffer);return wasmFusedMatMul(c,I,r.shape.length,M,A,s.shape.length,o,l,m,h,b,i||0,g),F}export const _fusedMatMulConfig={kernelName:_FusedMatMul,backendName:"wasm",setupFunc:setup,kernelFunc:fusedBatchMatMul};